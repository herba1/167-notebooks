{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/herba1/167-notebooks/blob/main/c166f25_02b_dqn_pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 3 --- Final Test for 06.11.2025"
      ],
      "metadata": {
        "id": "XtOuIKifmmyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari,accept-rom-license]\n",
        "!pip install autorom\n",
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqL9333gml9l",
        "outputId": "05eb30ee-4938-465a-a1fa-7d3dcc64957c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "\u001b[33mWARNING: gymnasium 1.2.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\n",
            "Collecting autorom\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from autorom) (8.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from autorom) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (2025.8.3)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Installing collected packages: autorom\n",
            "Successfully installed autorom-0.6.1\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.8.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.7.0-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2qmVoVmxve",
        "outputId": "a759cf1e-c65f-4e1a-e5e2-2197fa36d37a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.12/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/et.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/zaxxon.bin\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the Gym"
      ],
      "metadata": {
        "id": "RQyqgMFzT-qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py\n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "_W_afhrzUAnp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure the model save drive"
      ],
      "metadata": {
        "id": "E0n7zQrALq7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v8hSrWrrLibc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cabd57f9-28b3-4ba8-9ca8-a9e00ae8b5bb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "save_dir = \"/content/drive/MyDrive/PUBLIC/Models\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "vlWPUjKfLv9y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Model"
      ],
      "metadata": {
        "id": "SAEvyoukL1T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import typing as tt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard.writer import SummaryWriter"
      ],
      "metadata": {
        "id": "VCbjkLLSxCUN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn_model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "    def forward(self, x: torch.ByteTensor):\n",
        "        x = x.float() / 255.0\n",
        "        return self.fc(self.conv(x))"
      ],
      "metadata": {
        "id": "dIJ32Rs6xJsV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wrappers\n",
        "\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    ImageToPyTorch: Reorders image dimensions from (H, W, C) to (C, H, W)\n",
        "    for compatibility with PyTorch convolutional layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        obs = self.observation_space\n",
        "        assert isinstance(obs, gym.spaces.Box)\n",
        "        assert len(obs.shape) == 3\n",
        "        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=obs.low.min(), high=obs.high.max(),\n",
        "            shape=new_shape, dtype=obs.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    BufferWrapper: Maintains a rolling window of the last `n_steps` frames\n",
        "    to give the agent a sense of temporal context.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            dtype=obs.dtype)\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen):\n",
        "            self.buffer.append(np.zeros_like(self.env.observation_space.low))\n",
        "        obs, extra = self.env.reset()\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, n_steps=4, render_mode=None, **kwargs):\n",
        "    print(f\"Creating environment {env_name}\")\n",
        "    env = gym.make(env_name, render_mode=render_mode, **kwargs)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=n_steps)\n",
        "    return env"
      ],
      "metadata": {
        "id": "Vk2CtGcOBcQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7565f05e-d03c-4b9e-d80f-98cebee374a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base Configuration\n",
        "DEFAULT_ENV_NAME = \"ALE/Boxing-v5\"\n",
        "MEAN_REWARD_BOUND = 19\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "SAVE_EPSILON = 0.5  # Only save if at least this much better\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.01\n",
        "\n",
        "# Tuple of tensors returned from a sampled minibatch in replay buffer\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    torch.ByteTensor,           # current state\n",
        "    torch.LongTensor,           # actions\n",
        "    torch.Tensor,               # rewards\n",
        "    torch.BoolTensor,           # done || trunc\n",
        "    torch.ByteTensor            # next state\n",
        "]"
      ],
      "metadata": {
        "id": "GvXPdjPCBxOd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⚙️ Fast Training Config for Quick Test Run\n",
        "MEAN_REWARD_BOUND = 5\n",
        "REPLAY_START_SIZE = 1000\n",
        "EPSILON_DECAY_LAST_FRAME = 10_000\n",
        "SYNC_TARGET_FRAMES = 500\n",
        "\n",
        "# REPLAY_SIZE = 5000  # optional\n",
        "# BATCH_SIZE = 16     # optional"
      ],
      "metadata": {
        "id": "3FHvHNMrR9Sk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Define directories\n",
        "save_dir_drive = \"/content/drive/MyDrive/PUBLIC/Models\"\n",
        "save_dir_local = \"saved_models\"\n",
        "\n",
        "# Create both directories if they don't exist\n",
        "os.makedirs(save_dir_drive, exist_ok=True)\n",
        "os.makedirs(save_dir_local, exist_ok=True)\n",
        "\n",
        "# Safe model filename\n",
        "env_name = DEFAULT_ENV_NAME\n",
        "safe_env_name = env_name.replace(\"/\", \"_\")"
      ],
      "metadata": {
        "id": "MkILCuT2OhBV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ],
      "metadata": {
        "id": "uW4Bo-mcB6rc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net: DQN, device: torch.device,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v.unsqueeze_(0)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "metadata": {
        "id": "irJb4V32B-R8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    states_t = torch.as_tensor(np.asarray(states))\n",
        "    actions_t = torch.LongTensor(actions)\n",
        "    rewards_t = torch.FloatTensor(rewards)\n",
        "    dones_t = torch.BoolTensor(dones)\n",
        "    new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "           dones_t.to(device),  new_states_t.to(device)"
      ],
      "metadata": {
        "id": "vHXmNr_wCBJm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "              device: torch.device) -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n",
        "\n",
        "    state_action_values = net(states_t).gather(\n",
        "        1, actions_t.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "    with torch.no_grad():\n",
        "        next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "        next_state_values[dones_t] = 0.0\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_t\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ],
      "metadata": {
        "id": "-dbh0431CEXO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_comment = f\"test_epsdec{EPSILON_DECAY_LAST_FRAME}_rs{REPLAY_START_SIZE}_sync{SYNC_TARGET_FRAMES}\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = make_env(env_name)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{env_name}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time  # in seconds\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        #  print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "        #      f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "        if best_m_reward is None or m_reward > best_m_reward + SAVE_EPSILON:\n",
        "            print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "                f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "\n",
        "            # Save to both paths\n",
        "            model_path_drive = os.path.join(save_dir_drive, model_filename)\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "            torch.save(net.state_dict(), model_path_drive)\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "            print(f\"💾 Model saved to:\\n - Google Drive: {model_path_drive}\\n - Local:        {model_path_local}\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "        if m_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "env.close()\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "Q8gj-5woCXEB",
        "outputId": "1da58808-63b1-467f-b889-df6679c7839d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating environment ALE/Boxing-v5\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=18, bias=True)\n",
            "  )\n",
            ")\n",
            "445: done 1 games, reward -11.000, eps 0.96, speed 228.36 f/s, time 0.0 min\n",
            "💾 Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_Boxing-v5-best_-11-20250929-0248-test_epsdec10000_rs1000_sync500.dat\n",
            " - Local:        saved_models/ALE_Boxing-v5-best_-11-20250929-0248-test_epsdec10000_rs1000_sync500.dat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3934743577.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPSILON_FINAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON_START\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mEPSILON_DECAY_LAST_FRAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2564180992.py\u001b[0m in \u001b[0;36mplay_step\u001b[0;34m(self, net, device, epsilon)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# do step in the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    558\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    559\u001b[0m         \u001b[0;34m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    558\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    559\u001b[0m         \u001b[0;34m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    325\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    326\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    558\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    559\u001b[0m         \u001b[0;34m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    325\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    326\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAtariStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mterminated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtruncated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    325\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    326\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ale_py/env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_truncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def make_video_to_drive(model_path, video_filename):\n",
        "    \"\"\"Save video directly to Google Drive\"\"\"\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "    env = make_env(\"ALE/Boxing-v5\", render_mode='rgb_array')\n",
        "\n",
        "    model = DQN(env.observation_space.shape, env.action_space.n)\n",
        "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    model.eval()\n",
        "\n",
        "    # Collect frames\n",
        "    frames = []\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done and len(frames) < 2000:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action = model(torch.as_tensor(obs).unsqueeze(0)).argmax().item()\n",
        "\n",
        "        obs, r, term, trunc, _ = env.step(action)\n",
        "        total_reward += r\n",
        "        done = term or trunc\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Save directly to Drive\n",
        "    if frames:\n",
        "        drive_path = f'/content/drive/MyDrive/PUBLIC/{video_filename}.mp4'\n",
        "        h, w, _ = frames[0].shape\n",
        "\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        writer = cv2.VideoWriter(drive_path, fourcc, 30, (w, h))\n",
        "\n",
        "        for frame in frames:\n",
        "            writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        writer.release()\n",
        "        print(f\"Saved: {drive_path}\")\n",
        "        print(f\"Stats: {len(frames)} frames, reward={total_reward}\")\n",
        "\n",
        "# Make both videos directly to Drive\n",
        "make_video_to_drive(\n",
        "    \"/content/drive/MyDrive/PUBLIC/Models/ALE_Boxing-v5-best_2-20250929-0028-test_epsdec10000_rs1000_sync500.dat\",\n",
        "    \"boxing_early\"\n",
        ")\n",
        "\n",
        "make_video_to_drive(\n",
        "    \"/content/drive/MyDrive/PUBLIC/Models/ALE_Boxing-v5-best_3-20250929-0148-test_epsdec10000_rs1000_sync500.dat\",\n",
        "    \"boxing_later\"\n",
        ")\n",
        "\n",
        "print(\"\\nVideos saved to your Drive in PUBLIC folder!\")\n",
        "print(\"Check: MyDrive/PUBLIC/boxing_early.mp4 and boxing_later.mp4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjbk7OVmOQhn",
        "outputId": "5e22822a-5671-4779-c1fa-3c1cee11447b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating environment ALE/Boxing-v5\n",
            "Saved: /content/drive/MyDrive/PUBLIC/boxing_early.mp4\n",
            "Stats: 315 frames, reward=-100.0\n",
            "Creating environment ALE/Boxing-v5\n",
            "Saved: /content/drive/MyDrive/PUBLIC/boxing_later.mp4\n",
            "Stats: 445 frames, reward=0.0\n",
            "\n",
            "Videos saved to your Drive in PUBLIC folder!\n",
            "Check: MyDrive/PUBLIC/boxing_early.mp4 and boxing_later.mp4\n"
          ]
        }
      ]
    }
  ]
}